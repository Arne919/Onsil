3834. 2차시. 간단한 뉴럴넷 모델 - PyTorch: Mini-batch SGD Lv1

// 이곳에 정답을 입력합니다.
Q1_1) learning_rate를 높이면 어떻게 될까요?
-> 학습 속도가 빨라지지만, 너무 높으면 손실 함수가 발산할 위험이 있습니다. 즉, 모델이 수렴하지 않고 최적의 해를 찾지 못할 수 있습니다.


Q1_2) 반대로 낮추면 어떻게 될까요?
-> 학습 속도가 느려지지만, 더 안정적인 학습이 가능합니다. 낮은 학습률은 손실 함수의 변화를 더 세밀하게 조정하여 최적화 가능성을 높일 수 있지만, 너무 낮으면 학습이 지나치게 오래 걸릴 수 있습니다.

Q2) 손실함수를 MSELoss가 아닌 다른 손실함수를 쓴다면 여러분의 선택은 어떤 함수를 쓰실 건가요?
-> 선택할 수 있는 다른 손실 함수로는 CrossEntropyLoss (분류 문제의 경우) 또는 Hinge Loss (서포트 벡터 머신에 사용됨) 등이 있습니다. 선택은 문제의 특성에 따라 달라집니다.

Q3_1) y_true = y_valid_tensor.squeeze().numpy()에서 y_valid_tensor를 squeeze하지 않았을때의 값의 출력은 어떻게 달라질까요?
-> squeeze()를 사용하지 않으면, 출력의 차원이 늘어나고 (예: (N, 1) 형태), 다차원 배열로 반환됩니다. 이는 불필요한 차원이 추가되어 후속 처리나 시각화에 불편함을 초래할 수 있습니다.

Q3_2) numpy하지 않았을때의 값의 출력은 어떻게 달라질까요?
-> numpy()를 사용하지 않으면, PyTorch의 텐서 형태로 그대로 유지됩니다. 이 경우 NumPy의 기능을 사용하여 배열 조작을 할 수 없게 됩니다.

Q3_3) 둘 다 하지 않았을때의 값의 출력은 어떻게 달라질까요?
-> squeeze()와 numpy()를 모두 사용하지 않으면, 텐서 형태로 남아 있으며 다차원 배열로 (예: (N, 1) 형태) 유지됩니다. 이를 통해 직접적으로 NumPy 배열의 기능을 사용할 수 없고, 후속 처리가 복잡해질 수 있습니다.

Q4) train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)에서 shuffle=True를 False로 바꾸면 어떤 영향을 끼칠까요?
-> shuffle=False로 바꾸면 데이터가 원래 순서대로 로드됩니다. 이 경우 모델이 특정 순서에 따라 데이터 패턴을 학습할 수 있어 과적합의 위험이 증가할 수 있습니다. 또한, 순차적인 데이터 특성이 있는 경우 (예: 시계열 데이터) 모델이 잘못된 관계를 학습할 수 있습니다.
이런 이유로 데이터 순서의 무작위성이 학습에 중요한 영향을 미칠 수 있습니다.
